# Full PostgreSQL DSN consumed by Go services (preferred)
POSTGRES_URL=postgres://postgres:postgres@172.18.0.3:5432/aro_hcp_embeddings?sslmode=disable

# Optional: Set log level
LOG_LEVEL=INFO

# PR Processing Configuration
# Maximum number of PRs to ingest per run (applies to both modes)
# Rate limit considerations:
#   - Unauthenticated: 60 API calls/hour (up to ~6,000 PRs/hour)
#   - Authenticated: 5,000 API calls/hour (up to ~500,000 PRs/hour)
# Recommended values:
#   - Development/testing: 100-500
#   - Production (unauthenticated): 1,000-5,000  
#   - Production (authenticated): 5,000-50,000
#   - Initial bulk import: 10,000+
INGESTION_LIMIT=50

# Ingestion mode
# INCREMENTAL: fetch PRs merged after the latest stored merge timestamp
# BATCH: ingest using the same parameters but respect direction
INGESTION_MODE=BATCH

# Start point for ingestion (ISO 8601). Used when the database is empty or for batch runs.
INGESTION_START_DATE=2025-09-01T00:00:00Z

# Batch mode configuration (only for INGESTION_MODE=BATCH)
BATCH_MODE_DIRECTION=onwards

# GitHub API Configuration (Phase 1B) - OPTIONAL for public repos
# For Azure/ARO-HCP (public repo): No token needed, 60 requests/hour
# With token: 5,000 requests/hour (recommended for heavy usage)
# GITHUB_TOKEN=your_github_token_here

# Database Recreation Configuration
# Controls whether to recreate database tables on startup
# Values: no, all, prs
# - no: Default, no tables dropped
# - all: Drop all database tables
RECREATE=no

# Ollama Configuration
# URL of the Ollama server for AI model inference
# Default: http://localhost:11434 (local Ollama instance)
# For remote GPU server: http://your-gpu-server:11434
OLLAMA_URL=http://192.168.0.10:11434

# ARO-HCP Image Tracer Configuration
# Path to the ARO-HCP repository clone (inside container)
ARO_HCP_REPO_PATH=/home/rvazquez/projects/ai-assisted-observability-poc/ignore/aro-hcp-repo

# Docker registry pull secret for accessing private container registries
# Should be a JSON string containing docker registry credentials
# Example: {"auths":{"registry.com":{"auth":"base64encodedcreds"}}}
# PULL_SECRET={"auths":{"arohcpsvcdev.azurecr.io":{"auth":"..."},"quay.io":{"auth":"..."}}}

# PR diff analyzer configuration
# EXECUTION_MODEL_NAME specifies the Ollama model to use for PR analysis (default: phi3)
# (Optional) Diff analyzer model (unused in current Go port)
# EXECUTION_MODEL_NAME=llama3.1:8b-instruct-q4_0
# PR_DIFF_CONTEXT_TOKENS controls the maximum context window in tokens for the PR analyzer (default: 4096)
PR_DIFF_CONTEXT_TOKENS=8192

# Embedding generation configuration
# EMBEDDING_MODEL_NAME sets the Ollama model used for embeddings (default: nomic-embed-text)
EMBEDDING_MODEL_NAME=nomic-embed-text

# MCP server HTTP binding
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000

# Diff analyzer configuration
DIFF_ANALYSIS_ENABLED=true
DIFF_ANALYSIS_MODEL=llama3.1:8b-instruct-q4_0
DIFF_ANALYSIS_OLLAMA_URL=http://192.168.0.10:11434
DIFF_ANALYSIS_CONTEXT_TOKENS=8192

# TRACE_IMAGES config options
PULL_SECRET=/home/rvazquez/projects/ai-assisted-observability-poc/ignore/pull-secret.json

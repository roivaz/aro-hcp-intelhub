# PostgreSQL Configuration
#POSTGRES_HOST=postgresql
POSTGRES_HOST=172.18.0.3
POSTGRES_PORT=5432
POSTGRES_DB=aro_hcp_embeddings
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# Optional: Set log level
LOG_LEVEL=INFO

# PR Processing Configuration
# Maximum number of PRs to ingest per run (both modes)
# Rate limit considerations:
#   - Unauthenticated: 60 API calls/hour (up to ~6,000 PRs/hour)
#   - Authenticated: 5,000 API calls/hour (up to ~500,000 PRs/hour)
# Recommended values:
#   - Development/testing: 100-500
#   - Production (unauthenticated): 1,000-5,000  
#   - Production (authenticated): 5,000-50,000
#   - Initial bulk import: 10,000+
MAX_NEW_PRS_PER_RUN=100

# Ingestion mode
# INCREMENTAL: process the newest PRs beyond the current high watermark
# BATCH: backfill older PRs below the current low watermark
INGESTION_MODE=BATCH

# Optional: Start ingesting PRs merged on or after this timestamp (ISO 8601)
# Examples:
#   - Entire history: leave unset
#   - Specific date: 2025-01-01T00:00:00Z
# Useful for batched backfills in home lab setups
PR_START_DATE=2025-01-01T00:00:00Z

# GitHub API Configuration (Phase 1B) - OPTIONAL for public repos
# For Azure/ARO-HCP (public repo): No token needed, 60 requests/hour
# With token: 5,000 requests/hour (recommended for heavy usage)
# GITHUB_TOKEN=your_github_token_here

# Database Recreation Configuration
# Controls whether to recreate database tables on startup
# Values: no, all, prs
# - no: Default, no tables dropped
# - all: Drop all PR embedding tables and processing state
# - prs: Drop only pr_embeddings table and clear PR state
RECREATE=all

# Ollama Configuration
# URL of the Ollama server for AI model inference
# Default: http://localhost:11434 (local Ollama instance)
# For remote GPU server: http://your-gpu-server:11434
OLLAMA_URL=http://192.168.0.10:11434

# ARO-HCP Image Tracer Configuration
# Path to the ARO-HCP repository clone (inside container)
ARO_HCP_REPO_PATH=/home/rvazquez/projects/ai-assisted-observability-poc/ignore/aro-hcp-repo

# Docker registry pull secret for accessing private container registries
# Should be a JSON string containing docker registry credentials
# Example: {"auths":{"registry.com":{"auth":"base64encodedcreds"}}}
# PULL_SECRET={"auths":{"arohcpsvcdev.azurecr.io":{"auth":"..."},"quay.io":{"auth":"..."}}}

# PR diff analyzer configuration
# EXECUTION_MODEL_NAME specifies the Ollama model to use for PR analysis (default: phi3)
EXECUTION_MODEL_NAME=llama3.1:8b-instruct-q4_0
# PR_DIFF_CONTEXT_TOKENS controls the maximum context window in tokens for the PR analyzer (default: 4096)
PR_DIFF_CONTEXT_TOKENS=8192

# Embedding generation configuration
# EMBEDDING_MODEL_NAME sets the Ollama model used for embeddings (default: nomic-embed-text)
EMBEDDING_MODEL_NAME=nomic-embed-text


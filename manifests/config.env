# Full PostgreSQL DSN consumed by Go services (preferred)
POSTGRES_URL=postgres://postgres:postgres@172.18.0.3:5432/aro_hcp_embeddings?sslmode=disable

# Optional: Set log level
LOG_LEVEL=INFO

# PR Processing Configuration - Two-phase architecture


# Execution mode
# FULL: Fetch from GitHub + process (embeddings + diff analysis) - Default, original behavior
# CACHE: Only fetch from GitHub and store metadata (no embeddings/analysis) - Fast, respects rate limits
# PROCESS: Only process unprocessed PRs from DB (embeddings + diff analysis) - Sequential processing
# 
# Recommended workflow:
#   1. CACHE mode: Quickly fetch thousands of PRs from GitHub incrementally
#   2. PROCESS mode: Process cached PRs sequentially when resources available
EXECUTION_MODE=FULL

# Processing configuration (for PROCESS mode)
# Maximum PRs to process from DB per run (0 = use GITHUB_FETCH_MAX)
MAX_PROCESS_BATCH=500

# GitHub fetch configuration (used by CACHE and FULL modes)
# Always fetches incrementally: resumes from latest PR in DB, or starts from GITHUB_FETCH_START_DATE if DB is empty

# Start point for GitHub fetching (ISO 8601). Only used when the database is empty.
GITHUB_FETCH_START_DATE=2000-01-01T00:00:00Z

# Maximum PRs to fetch from GitHub per run
# Rate limit considerations:
#   - Unauthenticated: 60 API calls/hour (up to ~6,000 PRs/hour)
#   - Authenticated: 5,000 API calls/hour (up to ~500,000 PRs/hour)
GITHUB_FETCH_MAX=5000

# GitHub API Configuration (Phase 1B) - OPTIONAL for public repos
# For Azure/ARO-HCP (public repo): No token needed, 60 requests/hour
# With token: 5,000 requests/hour (recommended for heavy usage)
# GITHUB_TOKEN=your_github_token_here

# Database Recreation Configuration
# Controls whether to recreate database tables on startup
# Values: no, all, prs
# - no: Default, no tables dropped
# - all: Drop all database tables
RECREATE=no

# Ollama Configuration
# URL of the Ollama server for AI model inference
# Default: http://localhost:11434 (local Ollama instance)
# For remote GPU server: http://your-gpu-server:11434
OLLAMA_URL=http://192.168.0.10:11434

# ARO-HCP Image Tracer Configuration
# Path to the ARO-HCP repository clone (inside container)
ARO_HCP_REPO_PATH=/home/rvazquez/projects/ai-assisted-observability-poc/ignore/aro-hcp-repo

# Docker registry pull secret for accessing private container registries
# Should be a JSON string containing docker registry credentials
# Example: {"auths":{"registry.com":{"auth":"base64encodedcreds"}}}
# PULL_SECRET={"auths":{"arohcpsvcdev.azurecr.io":{"auth":"..."},"quay.io":{"auth":"..."}}}

# PR diff analyzer configuration
# EXECUTION_MODEL_NAME specifies the Ollama model to use for PR analysis (default: phi3)
# (Optional) Diff analyzer model (unused in current Go port)
# EXECUTION_MODEL_NAME=llama3.1:8b-instruct-q4_0
# PR_DIFF_CONTEXT_TOKENS controls the maximum context window in tokens for the PR analyzer (default: 4096)
PR_DIFF_CONTEXT_TOKENS=8192

# Embedding generation configuration
# EMBEDDING_MODEL_NAME sets the Ollama model used for embeddings (default: nomic-embed-text)
EMBEDDING_MODEL_NAME=nomic-embed-text

# MCP server HTTP binding
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000

# Diff analyzer configuration
DIFF_ANALYSIS_ENABLED=true
DIFF_ANALYSIS_MODEL=llama3.1:8b-instruct-q4_0
DIFF_ANALYSIS_OLLAMA_URL=http://192.168.0.10:11434
DIFF_ANALYSIS_CONTEXT_TOKENS=8192

# TRACE_IMAGES config options
PULL_SECRET=/home/rvazquez/projects/ai-assisted-observability-poc/ignore/pull-secret.json

# Full PostgreSQL DSN consumed by Go services (preferred)
POSTGRES_URL=postgres://postgres:postgres@localhost:5432/aro_hcp_embeddings?sslmode=disable

# Optional: Set log level
LOG_LEVEL=INFO

# PR Processing Configuration - Two-phase architecture


# Execution mode
# FULL: Fetch from GitHub + process (embeddings + diff analysis) - Default, original behavior
# CACHE: Only fetch from GitHub and store metadata (no embeddings/analysis) - Fast, respects rate limits
# PROCESS: Only process unprocessed PRs from DB (embeddings + diff analysis) - Sequential processing
# 
# Recommended workflow:
#   1. CACHE mode: Quickly fetch thousands of PRs from GitHub incrementally
#   2. PROCESS mode: Process cached PRs sequentially when resources available
EXECUTION_MODE=FULL

# Processing configuration (for PROCESS mode)
# Maximum PRs to process from DB per run (0 = use GITHUB_FETCH_MAX)
MAX_PROCESS_BATCH=1000

# Maximum PRs to fetch from GitHub per run
# Rate limit considerations:
#   - Unauthenticated: 60 API calls/hour (up to ~6,000 PRs/hour)
#   - Authenticated: 5,000 API calls/hour (up to ~500,000 PRs/hour)
GITHUB_FETCH_MAX=5000

# GitHub API Configuration (Phase 1B) - OPTIONAL for public repos
# For Azure/ARO-HCP (public repo): No token needed, 60 requests/hour
# With token: 5,000 requests/hour (recommended for heavy usage)
# GITHUB_TOKEN=your_github_token_here

# Database Recreation Configuration
# Controls whether to recreate database tables on startup
# Values: no, all, prs
# - no: Default, no tables dropped
# - all: Drop all database tables
RECREATE=no

# Ollama Configuration
# URL of the Ollama server for AI model inference
# Default: http://localhost:11434 (local Ollama instance)
# For remote GPU server: http://your-gpu-server:11434
OLLAMA_URL=http://192.168.0.10:11434

# Cache Directory to clone repositories into
CACHE_DIR=/home/rvazquez/projects/ai-assisted-observability-poc/ignore

# PR diff analyzer configuration
# EXECUTION_MODEL_NAME specifies the Ollama model to use for PR analysis (default: phi3)
# (Optional) Diff analyzer model (unused in current Go port)
# EXECUTION_MODEL_NAME=llama3.1:8b-instruct-q4_0
# PR_DIFF_CONTEXT_TOKENS controls the maximum context window in tokens for the PR analyzer (default: 4096)
PR_DIFF_CONTEXT_TOKENS=8192

# Embedding generation configuration
# EMBEDDING_MODEL_NAME sets the Ollama model used for embeddings (default: nomic-embed-text)
EMBEDDING_MODEL_NAME=nomic-embed-text
# LLM call timeout applied to each Ollama request (Go duration, default 2m)
LLM_CALL_TIMEOUT=2m

# MCP server HTTP binding
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000

# Diff analyzer configuration
DIFF_ANALYSIS_ENABLED=true
DIFF_ANALYSIS_MODEL=llama3.1:8b-instruct-q4_0
DIFF_ANALYSIS_OLLAMA_URL=http://192.168.0.10:11434
DIFF_ANALYSIS_CONTEXT_TOKENS=8192

# TRACE_IMAGES config options
PULL_SECRET=/home/rvazquez/projects/ai-assisted-observability-poc/ignore/pull-secret.json

# Maximum cached trace_image responses to keep in Postgres (per commit/environment pair)
TRACE_CACHE_MAX_ENTRIES=500
